{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\MLOps-Project\\\\text-to-speech-using-mlops'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir : Path\n",
    "    wave_path : Path\n",
    "    csv_path : Path\n",
    "    sr  : int\n",
    "    n_fft : int\n",
    "    n_stft : int\n",
    "    frame_length : float\n",
    "    win_length : int\n",
    "    mel_freq : int\n",
    "    max_mel_time : int\n",
    "    max_db : int\n",
    "    scale_db : int\n",
    "    ref : float\n",
    "    power : float\n",
    "    norm_db : int\n",
    "    ampl_multiplier : float\n",
    "    ampl_amin : str\n",
    "    db_multiplier : float\n",
    "    ampl_ref : float\n",
    "    ampl_power : float\n",
    "    min_level_db : float\n",
    "    frame_shift : float\n",
    "    hop_length : int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.simpletts.constants import *\n",
    "from src.simpletts.utils.common import create_directories, read_yaml\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            wave_path = config.wave_path,\n",
    "            csv_path = config.csv_path,\n",
    "            sr  = self.params.sr,\n",
    "            n_fft = self.params.n_fft,\n",
    "            n_stft = self.params.n_stft,\n",
    "            frame_length = self.params.frame_length,\n",
    "            win_length = self.params.win_length,\n",
    "            mel_freq = self.params.mel_freq,\n",
    "            max_mel_time = self.params.max_mel_time,\n",
    "            max_db = self.params.max_db,\n",
    "            scale_db = self.params.scale_db,\n",
    "            ref = self.params.ref,\n",
    "            power = self.params.power,\n",
    "            norm_db = self.params.norm_db,\n",
    "            ampl_multiplier = self.params.ampl_multiplier,\n",
    "            ampl_amin = self.params.ampl_amin,\n",
    "            db_multiplier = self.params.db_multiplier,\n",
    "            ampl_ref = self.params.ampl_ref,\n",
    "            ampl_power = self.params.ampl_power,\n",
    "            min_level_db = self.params.min_level_db,\n",
    "            frame_shift=self.params.frame_shift,\n",
    "            hop_length=self.params.hop_length\n",
    "        )\n",
    "        \n",
    "        \n",
    "        return data_transformation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "symbols = [\n",
    "    'EOS', ' ', '!', ',', '-', '.', \\\n",
    "    ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', \\\n",
    "    'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', \\\n",
    "    'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', \\\n",
    "    'â', 'è', 'é', 'ê', 'ü', '’', '“', '”' \\\n",
    "  ]\n",
    "\n",
    "\n",
    "symbol_to_id = {\n",
    "  s: i for i, s in enumerate(symbols)\n",
    "}\n",
    "\n",
    "def mask_from_seq_lengths(\n",
    "    sequence_lengths: torch.Tensor, \n",
    "    max_length: int\n",
    ") -> torch.BoolTensor:\n",
    "   \n",
    "    # (batch_size, max_length)\n",
    "    ones = sequence_lengths.new_ones(sequence_lengths.size(0), max_length)\n",
    "    range_tensor = ones.cumsum(dim=1)\n",
    "    return sequence_lengths.unsqueeze(1) >= range_tensor \n",
    "  \n",
    "def text_to_seq(text):\n",
    "    text = text.lower()\n",
    "    seq = []\n",
    "    for s in text:\n",
    "        _id = symbol_to_id.get(s, None)\n",
    "        if _id is not None:\n",
    "            seq.append(_id)\n",
    "    seq.append(symbol_to_id['EOS'])\n",
    "    return torch.IntTensor(seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from torchaudio.functional import spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProcessor:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        \n",
    "        print(type(self.config.min_level_db))\n",
    "        print(type(self.config.max_db))\n",
    "        print(type(self.config.norm_db))\n",
    "        print(type(self.config.ref))\n",
    "\n",
    "        \n",
    "        self.spec_transform = torchaudio.transforms.Spectrogram(\n",
    "            n_fft=self.config.n_fft,\n",
    "            win_length=self.config.win_length,\n",
    "            hop_length=self.config.hop_length,\n",
    "            power=self.config.power\n",
    "        )\n",
    "        \n",
    "        self.mel_scale_transform = torchaudio.transforms.MelScale(\n",
    "            n_mels=self.config.mel_freq,\n",
    "            sample_rate=self.config.sr,\n",
    "            n_stft=self.config.n_stft\n",
    "        )\n",
    "        \n",
    "        self.mel_inverse_transform = torchaudio.transforms.InverseMelScale(\n",
    "            n_mels=self.config.mel_freq,\n",
    "            sample_rate=self.config.sr,\n",
    "            n_stft=self.config.n_stft\n",
    "        ).cuda()\n",
    "        \n",
    "        self.griffnlim_transform = torchaudio.transforms.GriffinLim(\n",
    "            n_fft=self.config.n_fft,\n",
    "            win_length=self.config.win_length,\n",
    "            hop_length=self.config.hop_length\n",
    "        ).cuda()\n",
    "        \n",
    "    def norm_mel_spec_db(mel_spec): \n",
    "        min_level_db = -100.0\n",
    "        max_db = 100\n",
    "        norm_db = 10\n",
    "        ref = 4.0\n",
    "        mel_spec = ((2.0*mel_spec - min_level_db) / (max_db/norm_db)) - 1.0\n",
    "        mel_spec = torch.clip(mel_spec, -ref*norm_db, ref*norm_db)\n",
    "        return mel_spec\n",
    "\n",
    "\n",
    "    \n",
    "    def denorm_mel_spec_db(self, mel_spec):\n",
    "        mel_spec = (((1.0 + mel_spec) * (self.config.max_db / self.config.norm_db)) + self.config.min_level_db) / 2.0\n",
    "        return mel_spec\n",
    "    \n",
    "    def pow_to_db_mel_spec(self, mel_spec):\n",
    "        mel_spec = torchaudio.functional.amplitude_to_DB(\n",
    "            mel_spec,\n",
    "            multiplier=self.config.ampl_multiplier,\n",
    "            amin=self.config.ampl_amin,\n",
    "            db_multiplier=self.config.db_multiplier,\n",
    "            top_db=self.config.max_db\n",
    "        )\n",
    "        mel_spec = mel_spec / self.config.scale_db\n",
    "        return mel_spec\n",
    "    \n",
    "    def db_to_power_mel_spec(self, mel_spec):\n",
    "        mel_spec = mel_spec * self.config.scale_db\n",
    "        mel_spec = torchaudio.functional.DB_to_amplitude(\n",
    "            mel_spec,\n",
    "            ref=self.config.ampl_ref,\n",
    "            power=self.config.ampl_power\n",
    "        )\n",
    "        return mel_spec\n",
    "    \n",
    "    def convert_to_mel_spec(self, wav):\n",
    "        spec = self.spec_transform(wav)\n",
    "        mel_spec = self.mel_scale_transform(spec)\n",
    "        db_mel_spec = self.pow_to_db_mel_spec(mel_spec)\n",
    "        db_mel_spec = db_mel_spec.squeeze(0)\n",
    "        return db_mel_spec\n",
    "    \n",
    "    def inverse_mel_spec_to_wav(self, mel_spec):\n",
    "        power_mel_spec = self.db_to_power_mel_spec(mel_spec)\n",
    "        spectrogram = self.mel_inverse_transform(power_mel_spec)\n",
    "        pseudo_wav = self.griffnlim_transform(spectrogram)\n",
    "        return pseudo_wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils\n",
    "\n",
    "\n",
    "class TextMelDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, config: DataTransformationConfig):\n",
    "        self.df = df\n",
    "        self.cache = {}\n",
    "        self.config = config\n",
    "        self.audio_processor = AudioProcessor(config)  # Pass the config here\n",
    "        \n",
    "    def get_item(self, row):\n",
    "        wav_id = row['wav']\n",
    "        wav_path = f\"{self.config.wave_path}/{wav_id}.wav\"\n",
    "        text = row['text_norm']\n",
    "        text = text_to_seq(text)\n",
    "        waveform, sample_rate = torchaudio.load(wav_path, normalize=True)\n",
    "        assert sample_rate == self.config.sr\n",
    "        mel = self.audio_processor.convert_to_mel_spec(waveform)\n",
    "        return (text, mel)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        wave_id = row['wav']\n",
    "        text_mel = self.cache.get(wave_id)\n",
    "        if text_mel is None:\n",
    "            text_mel = self.get_item(row)\n",
    "            self.cache[wave_id] = text_mel\n",
    "            \n",
    "        return text_mel\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def text_mel_collate_fn(batch):\n",
    "        text_length_max = torch.tensor(\n",
    "            [text.shape[-1] for text, _ in batch], \n",
    "            dtype=torch.int32\n",
    "        ).max()\n",
    "        mel_length_max = torch.tensor(\n",
    "            [mel.shape[-1] for _, mel in batch],\n",
    "            dtype=torch.int32\n",
    "        ).max()\n",
    "    \n",
    "        text_lengths = []\n",
    "        mel_lengths = []\n",
    "        texts_padded = []\n",
    "        mels_padded = []\n",
    "        for text, mel in batch:\n",
    "            text_length = text.shape[-1]      \n",
    "            text_padded = torch.nn.functional.pad(\n",
    "                text,\n",
    "                pad=[0, text_length_max-text_length],\n",
    "                value=0\n",
    "            )\n",
    "            mel_length = mel.shape[-1]\n",
    "            mel_padded = torch.nn.functional.pad(\n",
    "                mel,\n",
    "                pad=[0, mel_length_max-mel_length],\n",
    "                value=0\n",
    "            )\n",
    "            text_lengths.append(text_length)    \n",
    "            mel_lengths.append(mel_length)    \n",
    "            texts_padded.append(text_padded)    \n",
    "            mels_padded.append(mel_padded)\n",
    "        text_lengths = torch.tensor(text_lengths, dtype=torch.int32)\n",
    "        mel_lengths = torch.tensor(mel_lengths, dtype=torch.int32)\n",
    "        texts_padded = torch.stack(texts_padded, 0)\n",
    "        mels_padded = torch.stack(mels_padded, 0).transpose(1, 2)\n",
    "        stop_token_padded = mask_from_seq_lengths(\n",
    "            mel_lengths,\n",
    "            mel_length_max\n",
    "        )\n",
    "        stop_token_padded = (~stop_token_padded).float()\n",
    "        stop_token_padded[:, -1] = 1.0\n",
    "    \n",
    "        return texts_padded, \\\n",
    "            text_lengths, \\\n",
    "            mels_padded, \\\n",
    "            mel_lengths, \\\n",
    "            stop_token_padded\n",
    "\n",
    "            \n",
    "    \n",
    "            \n",
    "            \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.utils\n",
    "from src.simpletts.logging import logger\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def load_data(self):\n",
    "        df = pd.read_csv(self.config.csv_path)\n",
    "        return df\n",
    "    \n",
    "    def split_data(self, data):\n",
    "        train_df, test_df = train_test_split(data, test_size=0.4)\n",
    "        return train_df, test_df\n",
    "    \n",
    "    def create_dataset(self, train_df, test_df):\n",
    "        train_dataset = torch.utils.data.DataLoader(\n",
    "            TextMelDataset(train_df, self.config),\n",
    "            num_workers = 2,\n",
    "            shuffle = True,\n",
    "            sampler = None,\n",
    "            batch_size = 1,\n",
    "            pin_memory = True,\n",
    "            drop_last = True,\n",
    "            collate_fn = TextMelDataset.text_mel_collate_fn\n",
    "        )\n",
    "        \n",
    "        test_dataset = torch.utils.data.DataLoader(\n",
    "            TextMelDataset(test_df, self.config),\n",
    "            num_workers = 2,\n",
    "            shuffle = True,\n",
    "            sampler = None,\n",
    "            batch_size = 1,\n",
    "            pin_memory = True,\n",
    "            drop_last = True,\n",
    "            collate_fn = TextMelDataset.text_mel_collate_fn\n",
    "        )\n",
    "        \n",
    "        return  train_dataset, test_dataset\n",
    "        \n",
    "    \n",
    "    def save_datasets(self, train_dataset, test_dataset):\n",
    "        os.makedirs(self.config.root_dir, exist_ok=True)\n",
    "\n",
    "        train_dataset_path = os.path.join(self.config.root_dir, 'train_dataset.pt')\n",
    "        test_dataset_path = os.path.join(self.config.root_dir, 'test_dataset.pt')\n",
    "\n",
    "        try:\n",
    "            torch.save(train_dataset, train_dataset_path)\n",
    "            torch.save(test_dataset, test_dataset_path)\n",
    "\n",
    "            logger.info(f\"Train dataset saved at: {train_dataset_path}\")\n",
    "            logger.info(f\"Test dataset saved at: {test_dataset_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving datasets: {str(e)}\")\n",
    "            raise e\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-31 20:23:31,907: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-08-31 20:23:31,910: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-08-31 20:23:31,911: INFO: common: created directory at: artifacts]\n",
      "[2024-08-31 20:23:31,912: INFO: common: created directory at: artifacts/data_transformation]\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "[2024-08-31 20:23:34,243: INFO: 2220319521: Train dataset saved at: artifacts/data_transformation\\train_dataset.pt]\n",
      "[2024-08-31 20:23:34,244: INFO: 2220319521: Test dataset saved at: artifacts/data_transformation\\test_dataset.pt]\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'float'>\n",
      "Inspecting train_dataset:\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Initialize the ConfigurationManager and DataTransformation\n",
    "    config_manager = ConfigurationManager()\n",
    "    data_transformation_config = config_manager.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    \n",
    "    # Load the dataset\n",
    "    dataset = data_transformation.load_data()\n",
    "    \n",
    "    # Split the data into train and test datasets\n",
    "    train_df, test_df = data_transformation.split_data(dataset)\n",
    "    \n",
    "    # Create the datasets using the split data\n",
    "    train_dataset, test_dataset = data_transformation.create_dataset(train_df, test_df)\n",
    "    \n",
    "    # Save the datasets to files\n",
    "    data_transformation.save_datasets(train_dataset, test_dataset)\n",
    "    \n",
    "    # Initialize the AudioProcessor with the DataTransformationConfig\n",
    "    audio_processor = AudioProcessor(config=data_transformation_config)\n",
    "    \n",
    "    \n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred during data transformation: {str(e)}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azizu\\AppData\\Local\\Temp\\ipykernel_27508\\2211277112.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  saved_dataset = torch.load('D:\\\\MLOps-Project\\\\text-to-speech-using-mlops\\\\artifacts\\\\train_loader.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting train_dataset:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# First, let's define the collate function here in the main module\n",
    "def text_mel_collate_fn(batch):\n",
    "    text_length_max = torch.tensor(\n",
    "        [text.shape[-1] for text, _ in batch], \n",
    "        dtype=torch.int32\n",
    "    ).max()\n",
    "    mel_length_max = torch.tensor(\n",
    "        [mel.shape[-1] for _, mel in batch],\n",
    "        dtype=torch.int32\n",
    "    ).max()\n",
    "\n",
    "    text_lengths = []\n",
    "    mel_lengths = []\n",
    "    texts_padded = []\n",
    "    mels_padded = []\n",
    "    for text, mel in batch:\n",
    "        text_length = text.shape[-1]      \n",
    "        text_padded = torch.nn.functional.pad(\n",
    "            text,\n",
    "            pad=[0, text_length_max-text_length],\n",
    "            value=0\n",
    "        )\n",
    "        mel_length = mel.shape[-1]\n",
    "        mel_padded = torch.nn.functional.pad(\n",
    "            mel,\n",
    "            pad=[0, mel_length_max-mel_length],\n",
    "            value=0\n",
    "        )\n",
    "        text_lengths.append(text_length)    \n",
    "        mel_lengths.append(mel_length)    \n",
    "        texts_padded.append(text_padded)    \n",
    "        mels_padded.append(mel_padded)\n",
    "    text_lengths = torch.tensor(text_lengths, dtype=torch.int32)\n",
    "    mel_lengths = torch.tensor(mel_lengths, dtype=torch.int32)\n",
    "    texts_padded = torch.stack(texts_padded, 0)\n",
    "    mels_padded = torch.stack(mels_padded, 0).transpose(1, 2)\n",
    "    stop_token_padded = mask_from_seq_lengths(\n",
    "        mel_lengths,\n",
    "        mel_length_max\n",
    "    )\n",
    "    stop_token_padded = (~stop_token_padded).float()\n",
    "    stop_token_padded[:, -1] = 1.0\n",
    "\n",
    "    return texts_padded, \\\n",
    "        text_lengths, \\\n",
    "        mels_padded, \\\n",
    "        mel_lengths, \\\n",
    "        stop_token_padded\n",
    "\n",
    "# Now let's load the saved dataset and create a new DataLoader\n",
    "try:\n",
    "    # Load the saved dataset\n",
    "    saved_dataset = torch.load('D:\\\\MLOps-Project\\\\text-to-speech-using-mlops\\\\artifacts\\\\train_loader.pt')\n",
    "    \n",
    "    # Recreate the DataLoader\n",
    "    train_dataset = DataLoader(\n",
    "        saved_dataset,\n",
    "        num_workers=2,\n",
    "        shuffle=True,\n",
    "        batch_size=1,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=text_mel_collate_fn\n",
    "    )\n",
    "\n",
    "    print(\"Inspecting train_dataset:\")\n",
    "    for batch in train_dataset:\n",
    "        texts_padded, text_lengths, mels_padded, mel_lengths, stop_token_padded = batch\n",
    "        \n",
    "        print(f\"\\nBatch contents:\")\n",
    "        print(f\"1. texts_padded shape: {texts_padded.shape}\")\n",
    "        print(f\"   Sample text (indices): {texts_padded[0]}\")\n",
    "        print(f\"2. text_lengths: {text_lengths}\")\n",
    "        print(f\"3. mels_padded shape: {mels_padded.shape}\")\n",
    "        print(f\"   Sample mel spectrogram shape: {mels_padded[0].shape}\")\n",
    "        print(f\"4. mel_lengths: {mel_lengths}\")\n",
    "        print(f\"5. stop_token_padded shape: {stop_token_padded.shape}\")\n",
    "        print(f\"   Sample stop token: {stop_token_padded[0]}\")\n",
    "        break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
