{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\MLOps-Project\\\\text-to-speech-using-mlops'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelBuildingConfig:\n",
    "    root_dir : Path\n",
    "    text_num_embeddings : int\n",
    "    embedding_size : int\n",
    "    encoder_embedding_size  : int\n",
    "    dim_feedforward : int\n",
    "    postnet_embedding_size : int\n",
    "    encoder_kernel_size : int\n",
    "    postnet_kernel_size : int\n",
    "    num_heads : int\n",
    "    dropout : float\n",
    "    batch_first : bool\n",
    "    mel_freq : int\n",
    "    max_mel_time : int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.simpletts.constants import *\n",
    "from src.simpletts.utils.common import create_directories, read_yaml\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "        \n",
    "    def get_model_building_config(self) -> ModelBuildingConfig:\n",
    "        config = self.config.model_building\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_building_config = ModelBuildingConfig(\n",
    "            root_dir= config.root_dir,\n",
    "            text_num_embeddings = self.params.text_num_embeddings,\n",
    "            embedding_size = self.params.embedding_size,\n",
    "            encoder_embedding_size = self.params.encoder_embedding_size,\n",
    "            dim_feedforward = self.params.dim_feedforward,\n",
    "            postnet_embedding_size = self.params.postnet_embedding_size,\n",
    "            encoder_kernel_size = self.params.encoder_kernel_size,\n",
    "            postnet_kernel_size = self.params.postnet_kernel_size,\n",
    "            dropout=self.params.dropout,\n",
    "            num_heads=self.params.num_heads,\n",
    "            batch_first = self.params.batch_first,\n",
    "            mel_freq = self.params.mel_freq,\n",
    "            max_mel_time = self.params.max_mel_time\n",
    "        )\n",
    "        \n",
    "        return model_building_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Represents a single encoder block in the Transformer architecture.\n",
    "\n",
    "    This block consists of self-attention followed by a feedforward network,\n",
    "    with layer normalization and residual connections.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config : ModelBuildingConfig):\n",
    "        \"\"\"\n",
    "        Initialize the EncoderBlock with its layers.\n",
    "        \"\"\"\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.config = config\n",
    "        self.norm_1 = nn.LayerNorm(normalized_shape=self.config.embedding_size)\n",
    "        self.attn = torch.nn.MultiheadAttention(\n",
    "            embed_dim= self.config.embedding_size,\n",
    "            num_heads=4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout_1 = torch.nn.Dropout(0.1)\n",
    "        self.norm_2 = nn.LayerNorm(normalized_shape= self.config.embedding_size)\n",
    "        self.linear_1 = nn.Linear( self.config.embedding_size,  self.config.dim_feedforward)\n",
    "        self.dropout_2 = torch.nn.Dropout(0.1)\n",
    "        self.linear_2 = nn.Linear(self.config.dim_feedforward, self.config.embedding_size)\n",
    "        self.dropout_3 = torch.nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the EncoderBlock.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor\n",
    "            attn_mask (Tensor, optional): Attention mask\n",
    "            key_padding_mask (Tensor, optional): Key padding mask\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after passing through the encoder block\n",
    "        \"\"\"\n",
    "        x_out = self.norm_1(x)\n",
    "        x_out, _ = self.attn(\n",
    "            query=x_out, \n",
    "            key=x_out, \n",
    "            value=x_out,\n",
    "            attn_mask=attn_mask,\n",
    "            key_padding_mask=key_padding_mask\n",
    "        )\n",
    "        x_out = self.dropout_1(x_out)\n",
    "        x = x + x_out    \n",
    "\n",
    "        x_out = self.norm_2(x) \n",
    "        x_out = self.linear_1(x_out)\n",
    "        x_out = F.relu(x_out)\n",
    "        x_out = self.dropout_2(x_out)\n",
    "        x_out = self.linear_2(x_out)\n",
    "        x_out = self.dropout_3(x_out)\n",
    "        x = x + x_out\n",
    "        \n",
    "        return x\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Represents a single decoder block in the Transformer architecture.\n",
    "\n",
    "    This block consists of self-attention, encoder-decoder attention, and a feedforward network,\n",
    "    with layer normalization and residual connections.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config : ModelBuildingConfig):\n",
    "        \"\"\"\n",
    "        Initialize the DecoderBlock with its layers.\n",
    "        \"\"\"\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.config = config\n",
    "        self.norm_1 = nn.LayerNorm(normalized_shape=self.config.embedding_size)\n",
    "        self.self_attn = torch.nn.MultiheadAttention(\n",
    "            embed_dim=self.config.embedding_size,\n",
    "            num_heads=self.config.num_heads,\n",
    "            dropout=self.config.dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout_1 = torch.nn.Dropout(0.1)\n",
    "        self.norm_2 = nn.LayerNorm(normalized_shape=self.config.embedding_size)\n",
    "        self.attn = torch.nn.MultiheadAttention(\n",
    "            embed_dim=self.config.embedding_size,\n",
    "            num_heads= self.config.num_heads,\n",
    "            dropout=self.config.dropout,\n",
    "            batch_first=True\n",
    "        )    \n",
    "        self.dropout_2 = torch.nn.Dropout(0.1)\n",
    "        self.norm_3 = nn.LayerNorm(normalized_shape=self.config.embedding_size)\n",
    "        self.linear_1 = nn.Linear(self.config.embedding_size, self.config.dim_feedforward)\n",
    "        self.dropout_3 = torch.nn.Dropout(self.config.dropout)\n",
    "        self.linear_2 = nn.Linear(self.config.dim_feedforward, self.config.embedding_size)\n",
    "        self.dropout_4 = torch.nn.Dropout(self.config.dropout)\n",
    "\n",
    "    def forward(self, x, memory, x_attn_mask=None, x_key_padding_mask=None,\n",
    "                memory_attn_mask=None, memory_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the DecoderBlock.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor\n",
    "            memory (Tensor): Encoder output\n",
    "            x_attn_mask (Tensor, optional): Self-attention mask\n",
    "            x_key_padding_mask (Tensor, optional): Self-attention key padding mask\n",
    "            memory_attn_mask (Tensor, optional): Encoder-decoder attention mask\n",
    "            memory_key_padding_mask (Tensor, optional): Encoder-decoder key padding mask\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after passing through the decoder block\n",
    "        \"\"\"\n",
    "        x_out, _ = self.self_attn(\n",
    "            query=x, \n",
    "            key=x, \n",
    "            value=x,\n",
    "            attn_mask=x_attn_mask,\n",
    "            key_padding_mask=x_key_padding_mask\n",
    "        )\n",
    "        x_out = self.dropout_1(x_out)\n",
    "        x = self.norm_1(x + x_out)\n",
    "         \n",
    "        x_out, _ = self.attn(\n",
    "            query=x,\n",
    "            key=memory,\n",
    "            value=memory,\n",
    "            attn_mask=memory_attn_mask,\n",
    "            key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "        x_out = self.dropout_2(x_out)\n",
    "        x = self.norm_2(x + x_out)\n",
    "\n",
    "        x_out = self.linear_1(x)\n",
    "        x_out = F.relu(x_out)\n",
    "        x_out = self.dropout_3(x_out)\n",
    "        x_out = self.linear_2(x_out)\n",
    "        x_out = self.dropout_4(x_out)\n",
    "        x = self.norm_3(x + x_out)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderPreNet(nn.Module):\n",
    "    def __init__(self, config: ModelBuildingConfig):\n",
    "        super(EncoderPreNet, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=config.text_num_embeddings,\n",
    "            embedding_dim=config.encoder_embedding_size\n",
    "        )\n",
    "        self.linear_1 = nn.Linear(config.encoder_embedding_size, config.encoder_embedding_size)\n",
    "        self.linear_2 = nn.Linear(config.encoder_embedding_size, config.embedding_size)\n",
    "        self.conv_1 = nn.Conv1d(\n",
    "            config.encoder_embedding_size, \n",
    "            config.encoder_embedding_size,\n",
    "            kernel_size=config.encoder_kernel_size, \n",
    "            stride=1,\n",
    "            padding=int((config.encoder_kernel_size - 1) / 2), \n",
    "            dilation=1\n",
    "        )\n",
    "        self.bn_1 = nn.BatchNorm1d(config.encoder_embedding_size)\n",
    "        self.dropout_1 = nn.Dropout(config.dropout)\n",
    "        self.conv_2 = nn.Conv1d(\n",
    "            config.encoder_embedding_size, \n",
    "            config.encoder_embedding_size,\n",
    "            kernel_size=config.encoder_kernel_size, \n",
    "            stride=1,\n",
    "            padding=int((config.encoder_kernel_size - 1) / 2), \n",
    "            dilation=1\n",
    "        )\n",
    "        self.bn_2 = nn.BatchNorm1d(config.encoder_embedding_size)\n",
    "        self.dropout_2 = nn.Dropout(config.dropout)\n",
    "        self.conv_3 = nn.Conv1d(\n",
    "            config.encoder_embedding_size, \n",
    "            config.encoder_embedding_size,\n",
    "            kernel_size=config.encoder_kernel_size, \n",
    "            stride=1,\n",
    "            padding=int((config.encoder_kernel_size - 1) / 2), \n",
    "            dilation=1\n",
    "        )\n",
    "        self.bn_3 = nn.BatchNorm1d(config.encoder_embedding_size)\n",
    "        self.dropout_3 = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        \"\"\"\n",
    "        Forward pass of the EncoderPreNet.\n",
    "\n",
    "        Args:\n",
    "            text (Tensor): Input text tensor\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Processed text tensor\n",
    "        \"\"\"\n",
    "        x = self.embedding(text) # (N, S, E)\n",
    "        x = self.linear_1(x)\n",
    "        x = x.transpose(2, 1) # (N, E, S) \n",
    "        x = self.conv_1(x)\n",
    "        x = self.bn_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.bn_2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.conv_3(x)\n",
    "        x = self.bn_3(x)    \n",
    "        x = F.relu(x)\n",
    "        x = self.dropout_3(x)\n",
    "        x = x.transpose(1, 2) # (N, S, E)\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Post-network that refines the output of the decoder.\n",
    "\n",
    "    This network applies a series of convolutional layers to the mel spectrogram.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config : ModelBuildingConfig):\n",
    "        \"\"\"\n",
    "        Initialize the PostNet with its layers.\n",
    "        \"\"\"\n",
    "        super(PostNet, self).__init__()  \n",
    "        self.config = config\n",
    "        self.conv_1 = nn.Conv1d(\n",
    "            self.config.mel_freq, \n",
    "            self.config.postnet_embedding_size,\n",
    "            kernel_size=self.config.postnet_kernel_size, \n",
    "            stride=1,\n",
    "            padding=int((self.config.postnet_kernel_size - 1) / 2), \n",
    "            dilation=1\n",
    "        )\n",
    "        self.bn_1 = nn.BatchNorm1d(self.config.postnet_embedding_size)\n",
    "        self.dropout_1 = torch.nn.Dropout(0.5)\n",
    "        self.conv_2 = nn.Conv1d(\n",
    "            self.config.postnet_embedding_size, \n",
    "            self.config.postnet_embedding_size,\n",
    "            kernel_size=self.config.postnet_kernel_size, \n",
    "            stride=1,\n",
    "            padding=int((self.config.postnet_kernel_size - 1) / 2), \n",
    "            dilation=1\n",
    "        )\n",
    "        self.bn_2 = nn.BatchNorm1d(self.config.postnet_embedding_size)\n",
    "        self.dropout_2 = torch.nn.Dropout(0.5)\n",
    "        self.conv_3 = nn.Conv1d(\n",
    "            self.config.postnet_embedding_size, \n",
    "            self.config.postnet_embedding_size,\n",
    "            kernel_size=self.config.postnet_kernel_size, \n",
    "            stride=1,\n",
    "            padding=int((self.config.postnet_kernel_size - 1) / 2), \n",
    "            dilation=1\n",
    "        )\n",
    "        self.bn_3 = nn.BatchNorm1d(self.config.postnet_embedding_size)\n",
    "        self.dropout_3 = torch.nn.Dropout(0.5)\n",
    "        self.conv_4 = nn.Conv1d(\n",
    "            self.config.postnet_embedding_size, \n",
    "            self.config.postnet_embedding_size,\n",
    "            kernel_size=self.config.postnet_kernel_size, \n",
    "            stride=1,\n",
    "            padding=int((self.config.postnet_kernel_size - 1) / 2), \n",
    "            dilation=1\n",
    "        )\n",
    "        self.bn_4 = nn.BatchNorm1d(self.config.postnet_embedding_size)\n",
    "        self.dropout_4 = torch.nn.Dropout(0.5)\n",
    "        self.conv_5 = nn.Conv1d(\n",
    "            self.config.postnet_embedding_size, \n",
    "            self.config.postnet_embedding_size,\n",
    "            kernel_size=self.config.postnet_kernel_size, \n",
    "            stride=1,\n",
    "            padding=int((self.config.postnet_kernel_size - 1) / 2), \n",
    "            dilation=1\n",
    "        )\n",
    "        self.bn_5 = nn.BatchNorm1d(self.config.postnet_embedding_size)\n",
    "        self.dropout_5 = torch.nn.Dropout(0.5)\n",
    "        self.conv_6 = nn.Conv1d(\n",
    "            self.config.postnet_embedding_size, \n",
    "            self.config.mel_freq,\n",
    "            kernel_size=self.config.postnet_kernel_size, \n",
    "            stride=1,\n",
    "            padding=int((self.config.postnet_kernel_size - 1) / 2), \n",
    "            dilation=1\n",
    "        )\n",
    "        self.bn_6 = nn.BatchNorm1d(self.config.mel_freq)\n",
    "        self.dropout_6 = torch.nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the PostNet.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input mel spectrogram\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Refined mel spectrogram\n",
    "        \"\"\"\n",
    "        x = x.transpose(2, 1) # (N, FREQ, TIME)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.bn_1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout_1(x) # (N, POSNET_DIM, TIME)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.bn_2(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout_2(x) # (N, POSNET_DIM, TIME)\n",
    "        x = self.conv_3(x)\n",
    "        x = self.bn_3(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout_3(x) # (N, POSNET_DIM, TIME)    \n",
    "        x = self.conv_4(x)\n",
    "        x = self.bn_4(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout_4(x) # (N, POSNET_DIM, TIME)    \n",
    "        x = self.conv_5(x)\n",
    "        x = self.bn_5(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout_5(x) # (N, POSNET_DIM, TIME)\n",
    "        x = self.conv_6(x)\n",
    "        x = self.bn_6(x)\n",
    "        x = self.dropout_6(x) # (N, FREQ, TIME)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderPreNet(nn.Module):\n",
    "    def __init__(self, config : ModelBuildingConfig):\n",
    "        super(DecoderPreNet, self).__init__()\n",
    "        self.config = config \n",
    "        self.linear_1 = nn.Linear(self.config.mel_freq, self.config.embedding_size)\n",
    "        self.linear_2 = nn.Linear(self.config.embedding_size, self.config.embedding_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=True)\n",
    "        x = self.linear_2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=True)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderPreNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder pre-network that processes mel spectrograms before the main decoder.\n",
    "\n",
    "    This network applies linear transformations with dropout.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config : ModelBuildingConfig):\n",
    "        \"\"\"\n",
    "        Initialize the DecoderPreNet with its layers.\n",
    "        \"\"\"\n",
    "        super(DecoderPreNet, self).__init__()\n",
    "        self.config = config\n",
    "        self.linear_1 = nn.Linear(self.config.mel_freq, self.config.embedding_size)\n",
    "        self.linear_2 = nn.Linear(self.config.embedding_size, self.config.embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the DecoderPreNet.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input mel spectrogram\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Processed mel spectrogram\n",
    "        \"\"\"\n",
    "        x = self.linear_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=True)\n",
    "        x = self.linear_2(x)\n",
    "        x = F.relu(x)    \n",
    "        x = F.dropout(x, p=0.5, training=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.simpletts.components.data_transformation import mask_from_seq_lengths\n",
    "class TransformerTTS(nn.Module):\n",
    "    def __init__(self, config: ModelBuildingConfig, device: str = \"cuda\"):\n",
    "        super(TransformerTTS, self).__init__()\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "\n",
    "        # Changed: Using config parameter for all submodule initializations\n",
    "        self.encoder_prenet = EncoderPreNet(config)\n",
    "        self.decoder_prenet = DecoderPreNet(config)\n",
    "        self.postnet = PostNet(config)\n",
    "        self.pos_encoding = nn.Embedding(config.max_mel_time, config.embedding_size)\n",
    "        self.encoder_block_1 = EncoderBlock(config)\n",
    "        self.encoder_block_2 = EncoderBlock(config)\n",
    "        self.encoder_block_3 = EncoderBlock(config)\n",
    "        self.decoder_block_1 = DecoderBlock(config)\n",
    "        self.decoder_block_2 = DecoderBlock(config)\n",
    "        self.decoder_block_3 = DecoderBlock(config)\n",
    "        self.linear_1 = nn.Linear(config.embedding_size, config.mel_freq)\n",
    "        self.linear_2 = nn.Linear(config.embedding_size, 1)\n",
    "        self.norm_memory = nn.LayerNorm(config.embedding_size)\n",
    "\n",
    "    def forward(self, text, text_len, mel, mel_len):\n",
    "        \"\"\"\n",
    "        Forward pass of the TransformerTTS model.\n",
    "\n",
    "        Args:\n",
    "            text (Tensor): Input text tensor\n",
    "            text_len (Tensor): Lengths of input texts\n",
    "            mel (Tensor): Target mel spectrogram\n",
    "            mel_len (Tensor): Lengths of target mel spectrograms\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor, Tensor]: Predicted mel spectrogram (post-net), \n",
    "                                           predicted mel spectrogram (pre-net),\n",
    "                                           stop token predictions\n",
    "        \"\"\"\n",
    "        N = text.shape[0]\n",
    "        S = text.shape[1]\n",
    "        TIME = mel.shape[1]\n",
    "\n",
    "        # Create masks\n",
    "        self.src_key_padding_mask = torch.zeros((N, S), device=text.device).masked_fill(\n",
    "            ~mask_from_seq_lengths(text_len, max_length=S), float(\"-inf\")\n",
    "        )\n",
    "        self.src_mask = torch.zeros((S, S), device=text.device).masked_fill(\n",
    "            torch.triu(torch.full((S, S), True, dtype=torch.bool), diagonal=1).to(text.device),       \n",
    "            float(\"-inf\")\n",
    "        )\n",
    "        self.tgt_key_padding_mask = torch.zeros((N, TIME), device=mel.device).masked_fill(\n",
    "            ~mask_from_seq_lengths(mel_len, max_length=TIME), float(\"-inf\")\n",
    "        )\n",
    "        self.tgt_mask = torch.zeros((TIME, TIME), device=mel.device).masked_fill(\n",
    "            torch.triu(torch.full((TIME, TIME), True, device=mel.device, dtype=torch.bool), diagonal=1),       \n",
    "            float(\"-inf\")\n",
    "        )\n",
    "        self.memory_mask = torch.zeros((TIME, S), device=mel.device).masked_fill(\n",
    "            torch.triu(torch.full((TIME, S), True, device=mel.device, dtype=torch.bool), diagonal=1),       \n",
    "            float(\"-inf\")\n",
    "        )    \n",
    "\n",
    "        # Encoder\n",
    "        text_x = self.encoder_prenet(text)\n",
    "        pos_codes = self.pos_encoding(torch.arange(hp.max_mel_time).to(mel.device))\n",
    "        S = text_x.shape[1]\n",
    "        text_x = text_x + pos_codes[:S]\n",
    "        text_x = self.encoder_block_1(text_x, attn_mask=self.src_mask, key_padding_mask=self.src_key_padding_mask)\n",
    "        text_x = self.encoder_block_2(text_x, attn_mask=self.src_mask, key_padding_mask=self.src_key_padding_mask)\n",
    "        text_x = self.encoder_block_3(text_x, attn_mask=self.src_mask, key_padding_mask=self.src_key_padding_mask)\n",
    "        text_x = self.norm_memory(text_x)\n",
    "        \n",
    "        # Decoder\n",
    "        mel_x = self.decoder_prenet(mel)\n",
    "        mel_x = mel_x + pos_codes[:TIME]\n",
    "        mel_x = self.decoder_block_1(x=mel_x, memory=text_x, x_attn_mask=self.tgt_mask, \n",
    "                                     x_key_padding_mask=self.tgt_key_padding_mask,\n",
    "                                     memory_attn_mask=self.memory_mask,\n",
    "                                     memory_key_padding_mask=self.src_key_padding_mask)\n",
    "        mel_x = self.decoder_block_2(x=mel_x, memory=text_x, x_attn_mask=self.tgt_mask, \n",
    "                                     x_key_padding_mask=self.tgt_key_padding_mask,\n",
    "                                     memory_attn_mask=self.memory_mask,\n",
    "                                     memory_key_padding_mask=self.src_key_padding_mask)\n",
    "        mel_x = self.decoder_block_3(x=mel_x, memory=text_x, x_attn_mask=self.tgt_mask, \n",
    "                                     x_key_padding_mask=self.tgt_key_padding_mask,\n",
    "                                     memory_attn_mask=self.memory_mask,\n",
    "                                     memory_key_padding_mask=self.src_key_padding_mask)\n",
    "\n",
    "        # Output processing\n",
    "        mel_linear = self.linear_1(mel_x)\n",
    "        mel_postnet = self.postnet(mel_linear)\n",
    "        mel_postnet = mel_linear + mel_postnet\n",
    "        stop_token = self.linear_2(mel_x)\n",
    "\n",
    "        # Masking\n",
    "        bool_mel_mask = self.tgt_key_padding_mask.ne(0).unsqueeze(-1).repeat(1, 1, hp.mel_freq)\n",
    "        mel_linear = mel_linear.masked_fill(bool_mel_mask, 0)\n",
    "        mel_postnet = mel_postnet.masked_fill(bool_mel_mask, 0)\n",
    "        stop_token = stop_token.masked_fill(bool_mel_mask[:, :, 0].unsqueeze(-1), 1e3).squeeze(2)\n",
    "        \n",
    "        return mel_postnet, mel_linear, stop_token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "class BuildModel:\n",
    "    def __init__(self, config: ModelBuildingConfig):\n",
    "        self.config = config\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    def build(self):\n",
    "        model = TransformerTTS(config=self.config, device=self.device)\n",
    "        return model\n",
    "\n",
    "    def save_model(self, model: nn.Module, file_name: str = \"model.pth\"):\n",
    "        root_dir = str(self.config.root_dir)\n",
    "        save_path = os.path.join(root_dir, file_name)\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"Model saved to {save_path}\")\n",
    "\n",
    "    def build_and_save(self):\n",
    "        model = self.build()\n",
    "        self.save_model(model)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-03 20:26:30,191: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-09-03 20:26:30,197: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-09-03 20:26:30,200: INFO: common: created directory at: artifacts]\n",
      "[2024-09-03 20:26:30,202: INFO: common: created directory at: artifacts/model]\n",
      "Model saved to artifacts/model\\model.pth\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config_manager = ConfigurationManager()\n",
    "    model_config = config_manager.get_model_building_config()\n",
    "    model_builder = BuildModel(config=model_config)\n",
    "    model = model_builder.build_and_save()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
